{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199643ad",
   "metadata": {},
   "source": [
    "# PR Study 1 - Sample Size = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc75a8",
   "metadata": {},
   "source": [
    "### 'calc_action_values' generates action values via SR, PR \n",
    "- Input data: instructed goals, instructed reward, the SR & PR matrices, the action (0 or 1), mapping of states to columns of SR/PR matrix\n",
    "- Input algorithm: how planning is carried out \n",
    "    - one_or_many = is one goal or all gals used in the computation\n",
    "        - one_type refers to how the single goal is selected\n",
    "            - options are 'rwd','ev','prob','bin'\n",
    "    - sr_or_pr = is sr or pr used. Answers are \"sr\", \"pr\" or \"both\", the latter reflecting hybridization\n",
    "    - neglect_reward = neglect small reward\n",
    "    - neglect_rare_transition = neglect rare transitions\n",
    "    - inverse_prior_confidence = strength of prior that state transition is close to 0\n",
    "        - ratios = relative frequencies of transitions in dictionary\n",
    "            - to get ratios, use \"ret_relative_frequencies\" function (input: data file from study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3b5ff5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def individual_av_components(action=0,goals_reward={},goals_ratio={},sr_matrix=[],pr_matrix=[],state_map={},\n",
    "                       one_or_many='many',one_type='rwd',sr_or_pr='sr',neglect_reward='neg_rwd',\n",
    "                      neglect_rare_transition='neg_rare',\n",
    "                       inverse_prior_confidence=0,sr_pr_weighting=1,range_adapt='norm',nr_t=0.1,round_av=1,pr_comp='average'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #copy dictionaries to work on the copies only here\n",
    "    goals_ratios=goals_ratio.copy()\n",
    "    goals_rewards=goals_reward.copy()\n",
    "    \n",
    "    #load SR and PR matrices\n",
    "    pr_mat=np.load(pr_matrix)\n",
    "    pr_mat=np.transpose(pr_mat)\n",
    "    sr_mat=np.load(sr_matrix)\n",
    "\n",
    "    # determine whether SR , PR or Hybrid used\n",
    "    if sr_or_pr=='sr':\n",
    "        current_representation=sr_mat\n",
    "    elif sr_or_pr=='pr':\n",
    "        current_representation=pr_mat    \n",
    "            \n",
    "    \n",
    "    #get action name\n",
    "    if action==0:\n",
    "        action_name='trident'\n",
    "    elif action==1:\n",
    "        action_name='planet'\n",
    "    else:\n",
    "        print('wrong input for action -- can only be 0 or 1!')\n",
    "        return\n",
    "    \n",
    "    #remove goals if Neglect_Reward\n",
    "    goals_removed=[]\n",
    "    \n",
    "    if neglect_reward=='neg_rwd':\n",
    "        for key,value in goals_rewards.items():\n",
    "            if value<nr_t*max(goals_rewards.values()):\n",
    "                goals_removed.append(key)\n",
    "        for goal_r in goals_removed:\n",
    "            goals_rewards.pop(goal_r)\n",
    "            goals_ratios.pop(goal_r)\n",
    "    \n",
    "    max_goal=max(goals_rewards, key=goals_rewards.get)\n",
    "    max_prob=max(goals_ratios, key=goals_ratios.get)\n",
    "    goals_evs={}\n",
    "    for key,value in goals_rewards.items():\n",
    "        goals_evs[key]=current_representation[action,state_map[key]]*value\n",
    "    \n",
    "    max_ev=max(goals_evs, key=goals_evs.get)\n",
    "    #check if agent only selects one goal to focus on\n",
    "    if one_or_many=='one':\n",
    "        #only retain the highest-reward goal\n",
    "        if one_type=='rwd':\n",
    "            rem_list=[k for k in goals_rewards.keys() if k != max_goal]\n",
    "            goals_rewards = {key: goals_rewards[key]\n",
    "             for key in goals_rewards if key not in rem_list}\n",
    "            goals_ratios = {key: goals_ratios[key]\n",
    "             for key in goals_ratios if key not in rem_list}\n",
    "            \n",
    "        #only retain the highest-probability reward\n",
    "        elif one_type=='prob':\n",
    "            rem_list=[k for k in goals_ratios.keys() if k != max_prob]\n",
    "            goals_rewards = {key: goals_rewards[key]\n",
    "             for key in goals_rewards if key not in rem_list}\n",
    "            goals_ratios = {key: goals_ratios[key]\n",
    "             for key in goals_ratios if key not in rem_list}\n",
    "        \n",
    "        elif one_type=='ev':\n",
    "            rem_list=[k for k in goals_ratios.keys() if k != max_ev]\n",
    "            goals_rewards = {key: goals_rewards[key]\n",
    "             for key in goals_rewards if key not in rem_list}\n",
    "            goals_ratios = {key: goals_ratios[key]\n",
    "             for key in goals_ratios if key not in rem_list}\n",
    "\n",
    "    #gw is a placeholder so we can update goals_rewards dict by dividing by the max value \n",
    "    gw={}\n",
    "    if range_adapt=='norm':\n",
    "        for key,value in goals_rewards.items():\n",
    "            gw[key]=value/max(goals_rewards.values())\n",
    "        for key,value in goals_rewards.items():\n",
    "            goals_rewards[key]=gw[key]\n",
    "            \n",
    "    num_goals=len(goals_rewards.keys())\n",
    "\n",
    "    \n",
    "    # determine whether SR , PR or Hybrid used\n",
    "    if sr_or_pr=='sr':\n",
    "        current_representation=sr_mat\n",
    "        div_num_goals='no'\n",
    "        \n",
    "    elif sr_or_pr=='pr':\n",
    "        current_representation=pr_mat\n",
    "        div_num_goals='yes'\n",
    "    \n",
    "    # compute action values if for either SR or PR alone\n",
    "    if sr_or_pr=='sr' or sr_or_pr=='pr':\n",
    "        action_value=[0,0,0,0]\n",
    "        experience=0\n",
    "        reward_info=[0,0,0,0]\n",
    "        counter=0\n",
    "        for goal,reward in goals_rewards.items():\n",
    "\n",
    "            if div_num_goals=='yes':\n",
    "                reward=reward/num_goals\n",
    "            #neglect rare transitions\n",
    "            if neglect_rare_transition=='neg_rare' and neglect_reward=='':\n",
    "                if (current_representation[action,state_map[goal]])>(current_representation[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    if goals_ratios[goal][1]==0:\n",
    "                        action_value[counter]=0\n",
    "                        experience+=goals_ratios[goal][0]\n",
    "                        reward_info[counter]=reward\n",
    "                        counter+=1\n",
    "\n",
    "                    else:\n",
    "                        action_value[counter]=(goals_ratios[goal][0]/goals_ratios[goal][1])\n",
    "                        experience+=goals_ratios[goal][0]\n",
    "                        reward_info[counter]=reward\n",
    "                        counter+=1\n",
    "\n",
    "            elif neglect_rare_transition=='neg_rare' and neglect_reward=='neg_rwd':\n",
    "                if (current_representation[action,state_map[goal]])>(current_representation[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    if goals_ratios[goal][1]==0:\n",
    "                        action_value[counter]=0\n",
    "                        experience+=goals_ratios[goal][0]\n",
    "                        reward_info[counter]=reward\n",
    "                        counter+=1\n",
    "\n",
    "                    else:\n",
    "                        action_value[counter]=(goals_ratios[goal][0]/goals_ratios[goal][1])\n",
    "                        experience+=goals_ratios[goal][0]\n",
    "                        reward_info[counter]=1\n",
    "                        counter+=1\n",
    "\n",
    "                    \n",
    "            #consider rare transitions\n",
    "            elif neglect_rare_transition=='' and neglect_reward=='neg_rwd':\n",
    "                if goals_ratios[goal][1]==0:\n",
    "                        action_value+=0\n",
    "                else:\n",
    "                    action_value[counter]=(goals_ratios[goal][0]/goals_ratios[goal][1])\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "                    reward_info[counter]=1\n",
    "                    counter+=1\n",
    "\n",
    "            elif neglect_rare_transition=='' and neglect_reward=='':\n",
    "                if goals_ratios[goal][1]==0:\n",
    "                        action_value+=0\n",
    "                else:\n",
    "                    action_value[counter]=(goals_ratios[goal][0]/goals_ratios[goal][1])\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "                    reward_info[counter]=reward\n",
    "                    counter+=1\n",
    "\n",
    "        \n",
    "    ## compute action values if for hybrid agent\n",
    "    elif sr_or_pr=='both':\n",
    "        action_value=0\n",
    "        experience=0\n",
    "        for goal,reward in goals_rewards.items():\n",
    "            sr_probability=0\n",
    "            pr_probability=0\n",
    "            \n",
    "            #neglect rare transitions\n",
    "            if neglect_rare_transition=='neg_rare' and neglect_reward=='':\n",
    "                if (sr_mat[action,state_map[goal]])>(sr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    sr_probability=sr_matrix[action,state_map[goal]]\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "        \n",
    "                if (pr_mat[action,state_map[goal]])>(pr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    pr_probability=pr_matrix[action,state_map[goal]]\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "           \n",
    "        \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=reward*combined_probability\n",
    "                experience+=goals_ratios[goal][0]\n",
    "                    \n",
    "            elif neglect_rare_transition=='neg_rare' and neglect_reward=='neg_rwd':\n",
    "                if (sr_mat[action,state_map[goal]])>(sr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    sr_probability=sr_matrix[action,state_map[goal]]\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "                    \n",
    "                if (pr_mat[action,state_map[goal]])>(pr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    pr_probability=pr_matrix[action,state_map[goal]]\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "                \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=1*combined_probability\n",
    "                experience+=goals_ratios[goal][0]\n",
    "                    \n",
    "            #consider rare transitions\n",
    "            elif neglect_rare_transition=='' and neglect_reward=='':\n",
    "                sr_probability=sr_matrix[action,state_map[goal]]\n",
    "                pr_probability=pr_matrix[action,state_map[goal]]\n",
    "                \n",
    "                \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=reward*combined_probability\n",
    "                experience+=goals_ratios[goal][0]\n",
    "            \n",
    "            elif neglect_rare_transition=='' and neglect_reward=='neg_rwd':\n",
    "                sr_probability=sr_matrix[action,state_map[goal]]\n",
    "                pr_probability=pr_matrix[action,state_map[goal]]\n",
    "                \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=1*combined_probability\n",
    "                experience+=goals_ratios[goal][0]\n",
    "    \n",
    "\n",
    "    experience=experience/num_goals\n",
    "    \n",
    "    #action 1 is coded as negative, action 0 is coded as positive                  \n",
    "    if action==1:\n",
    "        action_value=action_value\n",
    "        experience=experience\n",
    "    if round_av==1:\n",
    "        action_value=[round(x,2) for x in action_value]\n",
    "        \n",
    "#     if pr_comp=='average':\n",
    "#         action_value=action_value/num_goals\n",
    "    return action_value,reward_info\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7e4fecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_action_values(action=0,goals_reward={},goals_ratio={},sr_matrix=[],pr_matrix=[],state_map={},\n",
    "                       one_or_many='many',one_type='rwd',sr_or_pr='sr',neglect_reward='neg_rwd',\n",
    "                      neglect_rare_transition='neg_rare',\n",
    "                       inverse_prior_confidence=0,sr_pr_weighting=1,range_adapt='norm',nr_t=0.1,round_av=1,pr_comp='average'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #copy dictionaries to work on the copies only here\n",
    "    goals_ratios=goals_ratio.copy()\n",
    "    goals_rewards=goals_reward.copy()\n",
    "    \n",
    "    #load SR and PR matrices\n",
    "    pr_mat=np.load(pr_matrix)\n",
    "    pr_mat=np.transpose(pr_mat)\n",
    "    sr_mat=np.load(sr_matrix)\n",
    "\n",
    "    # determine whether SR , PR or Hybrid used\n",
    "    if sr_or_pr=='sr':\n",
    "        current_representation=sr_mat\n",
    "    elif sr_or_pr=='pr':\n",
    "        current_representation=pr_mat    \n",
    "            \n",
    "    \n",
    "    #get action name\n",
    "    if action==0:\n",
    "        action_name='trident'\n",
    "    elif action==1:\n",
    "        action_name='planet'\n",
    "    else:\n",
    "        print('wrong input for action -- can only be 0 or 1!')\n",
    "        return\n",
    "    \n",
    "    #remove goals if Neglect_Reward\n",
    "    goals_removed=[]\n",
    "    \n",
    "    if neglect_reward=='neg_rwd':\n",
    "        for key,value in goals_rewards.items():\n",
    "            if value<nrt*max(goals_rewards.values()):\n",
    "                goals_removed.append(key)\n",
    "        for goal_r in goals_removed:\n",
    "            goals_rewards.pop(goal_r)\n",
    "            goals_ratios.pop(goal_r)\n",
    "    \n",
    "    max_goal=max(goals_rewards, key=goals_rewards.get)\n",
    "    max_prob=max(goals_ratios, key=goals_ratios.get)\n",
    "    goals_evs={}\n",
    "    for key,value in goals_rewards.items():\n",
    "        goals_evs[key]=current_representation[action,state_map[key]]*value\n",
    "    \n",
    "    max_ev=max(goals_evs, key=goals_evs.get)\n",
    "    #check if agent only selects one goal to focus on\n",
    "    if one_or_many=='one':\n",
    "        #only retain the highest-reward goal\n",
    "        if one_type=='rwd':\n",
    "            rem_list=[k for k in goals_rewards.keys() if k != max_goal]\n",
    "            goals_rewards = {key: goals_rewards[key]\n",
    "             for key in goals_rewards if key not in rem_list}\n",
    "            goals_ratios = {key: goals_ratios[key]\n",
    "             for key in goals_ratios if key not in rem_list}\n",
    "            \n",
    "        #only retain the highest-probability reward\n",
    "        elif one_type=='prob':\n",
    "            rem_list=[k for k in goals_ratios.keys() if k != max_prob]\n",
    "            goals_rewards = {key: goals_rewards[key]\n",
    "             for key in goals_rewards if key not in rem_list}\n",
    "            goals_ratios = {key: goals_ratios[key]\n",
    "             for key in goals_ratios if key not in rem_list}\n",
    "        \n",
    "        elif one_type=='ev':\n",
    "            rem_list=[k for k in goals_ratios.keys() if k != max_ev]\n",
    "            goals_rewards = {key: goals_rewards[key]\n",
    "             for key in goals_rewards if key not in rem_list}\n",
    "            goals_ratios = {key: goals_ratios[key]\n",
    "             for key in goals_ratios if key not in rem_list}\n",
    "\n",
    "    #gw is a placeholder so we can update goals_rewards dict by dividing by the max value \n",
    "    gw={}\n",
    "    if range_adapt=='norm':\n",
    "        for key,value in goals_rewards.items():\n",
    "            gw[key]=value/max(goals_rewards.values())\n",
    "        for key,value in goals_rewards.items():\n",
    "            goals_rewards[key]=gw[key]\n",
    "            \n",
    "    num_goals=len(goals_rewards.keys())\n",
    "\n",
    "    \n",
    "    # determine whether SR , PR or Hybrid used\n",
    "    if sr_or_pr=='sr':\n",
    "        current_representation=sr_mat\n",
    "        \n",
    "    elif sr_or_pr=='pr':\n",
    "        current_representation=pr_mat\n",
    "    \n",
    "    # compute action values if for either SR or PR alone\n",
    "    if sr_or_pr=='sr' or sr_or_pr=='pr':\n",
    "        action_value=0\n",
    "        experience=0\n",
    "        \n",
    "        for goal,reward in goals_rewards.items():\n",
    "            \n",
    "            #neglect rare transitions\n",
    "            if neglect_rare_transition=='neg_rare' and neglect_reward=='':\n",
    "                if (current_representation[action,state_map[goal]])>(current_representation[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    if goals_ratios[goal][1]==0:\n",
    "                        action_value+=0\n",
    "                    else:\n",
    "                        action_value+=(goals_ratios[goal][0]/goals_ratios[goal][1])*reward\n",
    "                        experience+=goals_ratios[goal][0]\n",
    "\n",
    "            elif neglect_rare_transition=='neg_rare' and neglect_reward=='neg_rwd':\n",
    "                if (current_representation[action,state_map[goal]])>(current_representation[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    if goals_ratios[goal][1]==0:\n",
    "                        action_value+=0\n",
    "                    else:\n",
    "                        action_value+=(goals_ratios[goal][0]/goals_ratios[goal][1])*1\n",
    "                        experience+=goals_ratios[goal][0]\n",
    "                    \n",
    "            #consider rare transitions\n",
    "            elif neglect_rare_transition=='' and neglect_reward=='neg_rwd':\n",
    "                if goals_ratios[goal][1]==0:\n",
    "                        action_value+=0\n",
    "                else:\n",
    "                    action_value+=(goals_ratios[goal][0]/goals_ratios[goal][1])*1\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "            elif neglect_rare_transition=='' and neglect_reward=='':\n",
    "                if goals_ratios[goal][1]==0:\n",
    "                        action_value+=0\n",
    "                else:\n",
    "                    action_value+=(goals_ratios[goal][0]/goals_ratios[goal][1])*reward\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "        \n",
    "    ## compute action values if for hybrid agent\n",
    "    elif sr_or_pr=='both':\n",
    "        action_value=0\n",
    "        experience=0\n",
    "        for goal,reward in goals_rewards.items():\n",
    "            sr_probability=0\n",
    "            pr_probability=0\n",
    "            \n",
    "            #neglect rare transitions\n",
    "            if neglect_rare_transition=='neg_rare' and neglect_reward=='':\n",
    "                if (sr_mat[action,state_map[goal]])>(sr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    sr_probability=sr_matrix[action,state_map[goal]]\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "        \n",
    "                if (pr_mat[action,state_map[goal]])>(pr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    pr_probability=pr_matrix[action,state_map[goal]]\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "           \n",
    "        \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=reward*combined_probability\n",
    "                experience+=goals_ratios[goal][0]\n",
    "                    \n",
    "            elif neglect_rare_transition=='neg_rare' and neglect_reward=='neg_rwd':\n",
    "                if (sr_mat[action,state_map[goal]])>(sr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    sr_probability=sr_matrix[action,state_map[goal]]\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "                    \n",
    "                if (pr_mat[action,state_map[goal]])>(pr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    pr_probability=pr_matrix[action,state_map[goal]]\n",
    "                    experience+=goals_ratios[goal][0]\n",
    "                \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=1*combined_probability\n",
    "                experience+=goals_ratios[goal][0]\n",
    "                    \n",
    "            #consider rare transitions\n",
    "            elif neglect_rare_transition=='' and neglect_reward=='':\n",
    "                sr_probability=sr_matrix[action,state_map[goal]]\n",
    "                pr_probability=pr_matrix[action,state_map[goal]]\n",
    "                \n",
    "                \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=reward*combined_probability\n",
    "                experience+=goals_ratios[goal][0]\n",
    "            \n",
    "            elif neglect_rare_transition=='' and neglect_reward=='neg_rwd':\n",
    "                sr_probability=sr_matrix[action,state_map[goal]]\n",
    "                pr_probability=pr_matrix[action,state_map[goal]]\n",
    "                \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=1*combined_probability\n",
    "                experience+=goals_ratios[goal][0]\n",
    "    \n",
    "\n",
    "    experience=experience/num_goals\n",
    "                    \n",
    "    #action 1 is coded as negative, action 0 is coded as positive                  \n",
    "    if action==1:\n",
    "        action_value=action_value*-1\n",
    "        experience=experience*-1\n",
    "    if round_av==1:\n",
    "        action_value=round(action_value,2)\n",
    "    if pr_comp=='average':\n",
    "        action_value=action_value/num_goals\n",
    "        \n",
    "    return action_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5a5afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_relative_frequencies(action=0,goals_rewards={},goals_ratios={},sr_matrix=[],pr_matrix=[],state_map={},\n",
    "                       one_or_many='many',one_type='rwd',sr_or_pr='sr',neglect_reward=1,\n",
    "                      neglect_rare_transition=1,inverse_prior_confidence=0,sr_pr_weighting=1):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    #get action name\n",
    "    if action==0:\n",
    "        action_name='trident'\n",
    "    elif action==1:\n",
    "        action_name='planet'\n",
    "    else:\n",
    "        print('wrong input for action -- can only be 0 or 1!')\n",
    "        return\n",
    "    \n",
    "    #remove goals if Neglect_Reward\n",
    "    goals_removed=[]\n",
    "    \n",
    "    if neglect_reward==1:\n",
    "        for key,value in goals_rewards.items():\n",
    "            if value<0.1*max(goals_rewards.values()):\n",
    "                goals_removed.append(key)\n",
    "        for goal_r in goals_removed:\n",
    "            goals_reward.pop(goal_r)\n",
    "            goals_ratios.pop(goal_r)\n",
    "    \n",
    "    max_goal=max(goals_rewards, key=goals_rewards.get)\n",
    "    max_prob=max(goals_ratios, key=goals_ratios.get)\n",
    "    \n",
    "    #check if agent only selects one goal to focus on\n",
    "    if one_or_many=='one':\n",
    "        #only retain the highest-reward goal\n",
    "        if one_type=='rwd':\n",
    "            rem_list=[k for k in goals_rewards.keys() if k != max_goal]\n",
    "            goals_rewards = {key: goals_rewards[key]\n",
    "             for key in goals_rewards if key not in rem_list}\n",
    "            goals_ratios = {key: goals_ratios[key]\n",
    "             for key in goals_ratios if key not in rem_list}\n",
    "            \n",
    "        #only retain the highest-probability reward\n",
    "        elif one_type=='prob':\n",
    "            rem_list=[k for k in goals_ratios.keys() if k != max_prob]\n",
    "            goals_rewards = {key: goals_rewards[key]\n",
    "             for key in goals_reward if key not in rem_list}\n",
    "            goals_ratios = {key: goals_ratios[key]\n",
    "             for key in goals_ratios if key not in rem_list}\n",
    "    \n",
    "    #load SR and PR matrices\n",
    "    pr_mat=np.load(pr_matrix)\n",
    "    sr_mat=np.load(sr_matrix)\n",
    "    \n",
    "    # determine whether SR , PR or Hybrid used\n",
    "    if sr_or_pr=='sr':\n",
    "        current_representation=sr_mat\n",
    "        \n",
    "    elif sr_or_pr=='pr':\n",
    "        current_representation=np.transpose(pr_mat)\n",
    "    \n",
    "    # compute action values if for either SR or PR alone\n",
    "    if sr_or_pr=='sr' or sr_or_pr=='pr':\n",
    "        action_value=0\n",
    "        \n",
    "        for goal,reward in goals_rewards.items():\n",
    "            \n",
    "            #neglect rare transitions\n",
    "            if neglect_rare_transition==1 and neglect_reward==0:\n",
    "                if (current_representation[action,state_map[goal]])>(current_representation[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    action_value+=(goals_ratios[goal][0]/(goals_ratios[goal][1]+inverse_prior_confidence))*reward\n",
    "            elif neglect_rare_transition==1 and neglect_reward==1:\n",
    "                if (current_representation[action,state_map[goal]])>(current_representation[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    action_value+=(goals_ratios[goal][0]/(goals_ratios[goal][1]+inverse_prior_confidence))*1\n",
    "            \n",
    "            #consider rare transitions\n",
    "            elif neglect_rare_transition==0 and neglect_reward==1:\n",
    "                action_value+=(goals_ratios[goal][0]/(goals_ratios[goal][1]+inverse_prior_confidence))*1\n",
    "                \n",
    "            elif neglect_rare_transition==0 and neglect_reward==0:\n",
    "                action_value+=(goals_ratios[goal][0]/(goals_ratios[goal][1]+inverse_prior_confidence))*reward\n",
    "            \n",
    "        \n",
    "    ## compute action values if for hybrid agent\n",
    "    elif sr_or_pr=='both':\n",
    "        action_value=0\n",
    "        \n",
    "        for goal,reward in goals_rewards.items():\n",
    "            sr_probability=0\n",
    "            pr_probability=0\n",
    "            \n",
    "            #neglect rare transitions\n",
    "            if neglect_rare_transition==1 and neglect_reward==0:\n",
    "                if (sr_mat[action,state_map[goal]])>(sr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    sr_probability=(goals_ratios[goal][0]/(goals_ratios[goal][1]+inverse_prior_confidence))\n",
    "                    \n",
    "                if (pr_mat[action,state_map[goal]])>(pr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    pr_probability=(goals_ratios[goal][2]/(goals_ratios[goal][3]+inverse_prior_confidence))\n",
    "                \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=reward*combined_probability\n",
    "                    \n",
    "            elif neglect_rare_transition==1 and neglect_reward==1:\n",
    "                if (sr_mat[action,state_map[goal]])>(sr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    sr_probability=(goals_ratios[goal][0]/(goals_ratios[goal][1]+inverse_prior_confidence))\n",
    "                    \n",
    "                if (pr_mat[action,state_map[goal]])>(pr_mat[int(np.abs(action-1)),state_map[goal]]):\n",
    "                    pr_probability=(goals_ratios[goal][2]/(goals_ratios[goal][3]+inverse_prior_confidence))\n",
    "                \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=1*combined_probability\n",
    "                    \n",
    "            #consider rare transitions\n",
    "            elif neglect_rare_transition==0 and neglect_reward==0:\n",
    "                sr_probability=(goals_ratios[goal][0]/(goals_ratios[goal][1]+inverse_prior_confidence))    \n",
    "                pr_probability=(goals_ratios[goal][2]/(goals_ratios[goal][3]+inverse_prior_confidence))\n",
    "                \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=reward*combined_probability\n",
    "            \n",
    "            elif neglect_rare_transition==0 and neglect_reward==1:\n",
    "                sr_probability=(goals_ratios[goal][0]/(goals_ratios[goal][1]+inverse_prior_confidence))    \n",
    "                pr_probability=(goals_ratios[goal][2]/(goals_ratios[goal][3]+inverse_prior_confidence))\n",
    "                \n",
    "                combined_probability = sr_pr_weighting*pr_probability+(1-sr_pr_weighting)*sr_probability\n",
    "                action_value+=1*combined_probability\n",
    "    \n",
    "        \n",
    "\n",
    "                \n",
    "                    \n",
    "    #action 1 is coded as negative, action 0 is coded as positive                  \n",
    "\n",
    "    return pr_probability,sr_probability\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0f0e8f",
   "metadata": {},
   "source": [
    "# Get the relative frequencies for SR and PR learned during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9848e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_frequencies(csv_file):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df=pd.read_csv(csv_file)\n",
    "    starting_states=df.s1_image.dropna().unique()\n",
    "    immediate_states=df.s2_image.dropna().unique()\n",
    "    distal_states=df.s3_image.dropna().unique()\n",
    "    \n",
    "    ratios_dictoinary={}\n",
    "    for im in immediate_states:\n",
    "        for st in starting_states:\n",
    "            ratios_dictoinary['{}_{}'.format(st[:-4],im[:-4])]=len(df[(df.s1_image.str.contains(st,na=False)) & (df.s2_image.str.contains(im,na=False))])\n",
    "    \n",
    "    for im in distal_states:\n",
    "        for st in starting_states:\n",
    "            ratios_dictoinary['{}_{}'.format(st[:-4],im[:-4])]=len(df[(df.s1_image.str.contains(st,na=False)) & (df.s3_image.str.contains(im,na=False))])\n",
    "            \n",
    "    return ratios_dictoinary        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333264ca",
   "metadata": {},
   "source": [
    "# Retrieve specific relative frequencies from dictionary of all relative frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ab70483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specific_ratio(ratio_dict,action_name,goals,sr_or_pr='sr'):\n",
    "    specific_ratio_dict={}\n",
    "    immediates=['tree','bell','fox','watch']\n",
    "    distals=['compass','houses','train','thermometer','microphone','snorkel','tophat','north']\n",
    "    for goal in goals:\n",
    "        total_denom_sr=0\n",
    "        goal_nom_sr=0\n",
    "        total_denom_pr=0\n",
    "        goal_nom_pr=0\n",
    "        \n",
    "        # Compute SR and PR relative frequencies experienced during learning\n",
    "        if goal in immediates:\n",
    "            for imm in immediates:\n",
    "                total_denom_sr+=ratio_dict['{}_{}'.format(action_name,imm)]\n",
    "                if imm==goal:\n",
    "                    goal_nom_sr=ratio_dict['{}_{}'.format(action_name,imm)]\n",
    "        elif goal in distals:\n",
    "            for dis in distals:\n",
    "                total_denom_sr+=ratio_dict['{}_{}'.format(action_name,dis)]\n",
    "                if dis==goal:\n",
    "                    goal_nom_sr=ratio_dict['{}_{}'.format(action_name,dis)]\n",
    "        for action_name_iter in ['trident','planet']:\n",
    "            total_denom_pr+=ratio_dict['{}_{}'.format(action_name_iter,goal)]    \n",
    "        goal_nom_pr=ratio_dict['{}_{}'.format(action_name,goal)]\n",
    "        #sr only\n",
    "        if sr_or_pr=='sr':\n",
    "            specific_ratio_dict[goal]=[goal_nom_sr,total_denom_sr]\n",
    "\n",
    "        #pr only\n",
    "        elif sr_or_pr=='pr':\n",
    "            specific_ratio_dict[goal]=[goal_nom_pr,total_denom_pr]\n",
    "\n",
    "        #hybrid\n",
    "        elif sr_or_pr=='both':\n",
    "            specific_ratio_dict[goal]=[goal_nom_sr,total_denom_sr,goal_nom_pr,total_denom_pr]\n",
    "\n",
    "    return specific_ratio_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffd928",
   "metadata": {},
   "source": [
    "# Data Study 1, get action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bfd426ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61460488fc1bdf4d2c8ce1db_pr_task_fullstudy1_2022-08-24_19h15.49.919.csv\n",
      "5e57a0020c3c6a14a1624031_pr_task_fullstudy1_2022-08-24_20h29.44.549.csv\n",
      "5b222aff59f9620001c109cb_pr_task_fullstudy1_2022-08-25_00h45.42.398.csv\n",
      "60ca1c436511b9fc8ab35615_pr_task_fullstudy1_2022-08-24_21h21.17.861.csv\n",
      "5e0fac24500f066c547f6376_pr_task_fullstudy1_2022-08-24_14h16.08.280.csv\n",
      "58404fd8ad98e40001ce915f_pr_task_fullstudy1_2022-08-24_19h18.19.531.csv\n",
      "5d811e6ab395350018786d78_pr_task_fullstudy1_2022-08-24_21h16.20.244.csv\n",
      "60951a228692926022c15fbd_pr_task_fullstudy1_2022-08-24_19h15.32.838.csv\n",
      "62e965d4ab0caa404c95a439_pr_task_fullstudy1_2022-08-24_19h15.35.456.csv\n",
      "5f07759f859dcb38b8d4c6b5_pr_task_fullstudy1_2022-08-24_21h15.46.754.csv\n",
      "5f4e97aae4fe380a7bad99c7_pr_task_fullstudy1_2022-08-24_20h07.57.755.csv\n",
      "5cc7443b836ddc0016fec706_pr_task_fullstudy1_2022-08-24_19h24.23.461.csv\n",
      "5fd7782dee03dc08d3f3f491_pr_task_fullstudy1_2022-08-25_01h04.37.751.csv\n",
      "60fd5adad03767cff6dddda8_pr_task_fullstudy1_2022-08-24_19h17.14.124.csv\n",
      "5f3287b83428cb05746e2214_pr_task_fullstudy1_2022-08-24_20h17.48.924.csv\n",
      "5685850c333cbd000d4e042f_pr_task_fullstudy1_2022-08-24_14h35.58.964.csv\n",
      "5e3c7f23a7f9e9078a2a009a_pr_task_fullstudy1_2022-08-24_19h18.34.135.csv\n",
      "60375d67dde85cad87f3c99d_pr_task_fullstudy1_2022-08-24_19h15.48.519.csv\n",
      "5e72db2782efd522c37d4a77_pr_task_fullstudy1_2022-08-24_15h07.34.884.csv\n",
      "5b258c9ba7cee100011d8aad_pr_task_fullstudy1_2022-08-24_19h15.43.783.csv\n",
      "5ddecbb55daaa6e4238aabc7_pr_task_fullstudy1_2022-08-24_19h53.41.773.csv\n",
      "5e93138a7d69590115fa1e21_pr_task_fullstudy1_2022-08-24_19h35.16.085.csv\n",
      "5f61341fe5033f0b6327fb7f_pr_task_fullstudy1_2022-08-24_20h15.52.879.csv\n",
      "601129f77e0c21000b0c408a_pr_task_fullstudy1_2022-08-25_08h51.35.414.csv\n",
      "5d96a61b5d0f1c00117201bd_pr_task_fullstudy1_2022-08-24_20h15.40.369.csv\n",
      "5dd58b43149f5354a6124a41_pr_task_fullstudy1_2022-08-24_19h52.15.616.csv\n",
      "60b0f00d76201e617923f93f_pr_task_fullstudy1_2022-08-24_19h21.27.423.csv\n",
      "615776777c08445c8ffd8d1b_pr_task_fullstudy1_2022-08-24_19h16.08.218.csv\n",
      "5abbc97de1099600016a286a_pr_task_fullstudy1_2022-08-24_19h15.40.561.csv\n",
      "603d75c3cd5c8ec37b0700a1_pr_task_fullstudy1_2022-08-24_20h40.09.315.csv\n",
      "5e7a2fc429c35307d86105a1_pr_task_fullstudy1_2022-08-24_20h26.21.241.csv\n",
      "60a6ba026f8bd75b67b23c97_pr_task_fullstudy1_2022-08-24_19h22.18.619.csv\n",
      "5ec3b9e83f187802a6aa041e_pr_task_fullstudy1_2022-08-24_21h36.37.683.csv\n",
      "5808ec7ff0740c0001789a1b_pr_task_fullstudy1_2022-08-24_19h15.58.813.csv\n",
      "615f338fdbb7744db42e0f7f_pr_task_fullstudy1_2022-08-24_19h15.38.145.csv\n",
      "62daeec8e79af298b60dcb83_pr_task_fullstudy1_2022-08-24_11h23.51.337.csv\n",
      "607081b0ecb2499634a349dd_pr_task_fullstudy1_2022-08-24_19h15.51.536.csv\n",
      "5b2a2d855b5c0900018b0bf5_pr_task_fullstudy1_2022-08-24_19h15.55.193.csv\n",
      "5e852dd84fc423121601a0f8_pr_task_fullstudy1_2022-08-24_20h18.06.931.csv\n",
      "5f4813c94e8b081877fb26a6_pr_task_fullstudy1_2022-08-24_14h16.02.686.csv\n",
      "5ecaa03045d07e0767b5d61e_pr_task_fullstudy1_2022-08-24_20h16.29.811.csv\n",
      "5e918b7bf9c9d14abca01622_pr_task_fullstudy1_2022-08-24_20h18.40.507.csv\n",
      "5fdf808af53c7a7e9199b430_pr_task_fullstudy1_2022-08-24_23h46.02.661.csv\n",
      "5c86d4f7d7c01f001660d938_pr_task_fullstudy1_2022-08-24_20h16.15.511.csv\n",
      "5b930339301eda0001ba2446_pr_task_fullstudy1_2022-08-24_20h19.04.448.csv\n",
      "5f20552d41aabd0e9fd6aab2_pr_task_fullstudy1_2022-08-24_16h07.32.001.csv\n",
      "613a469755c08ab3b9d682ad_pr_task_fullstudy1_2022-08-24_16h45.35.967.csv\n",
      "606241e3171463f6c3440f9e_pr_task_fullstudy1_2022-08-24_19h17.52.127.csv\n",
      "60c58250ee1f8bbfd27e5c98_pr_task_fullstudy1_2022-08-24_20h16.36.418.csv\n",
      "5eb6a07c52ea805a805893c2_pr_task_fullstudy1_2022-08-24_21h20.20.824.csv\n",
      "605b98c83da1ded576bcf096_pr_task_fullstudy1_2022-08-24_20h20.24.367.csv\n",
      "5dde680c40460edc4a5dc1a2_pr_task_fullstudy1_2022-08-24_20h15.37.721.csv\n",
      "60fc82ec40d698e0b433e1c1_pr_task_fullstudy1_2022-08-24_20h16.08.151.csv\n",
      "5e7a54cb26ed710245408b81_pr_task_fullstudy1_2022-08-24_20h21.35.908.csv\n",
      "5f70f2daea9a9f7076484982_pr_task_fullstudy1_2022-08-24_20h16.36.806.csv\n",
      "612ba6c594a6d54154a88ae7_pr_task_fullstudy1_2022-08-23_16h31.30.625.csv\n",
      "60a68725b2b424dc0d7ce793_pr_task_fullstudy1_2022-08-24_19h17.27.794.csv\n",
      "60513ec07ee46769ed029be4_pr_task_fullstudy1_2022-08-24_20h15.57.698.csv\n",
      "6012ed349807110d8eb941a6_pr_task_fullstudy1_2022-08-24_12h15.42.337.csv\n",
      "6266b7c4afe8a0ff03ca1f53_pr_task_fullstudy1_2022-08-24_19h22.27.433.csv\n",
      "5d29412ab711e9001ab74ece_pr_task_fullstudy1_2022-08-24_20h17.07.497.csv\n",
      "5c4f65bf2090bd0001c43421_pr_task_fullstudy1_2022-08-24_19h16.01.220.csv\n",
      "545d347afdf99b7f9fca22c7_pr_task_fullstudy1_2022-08-25_06h02.21.323.csv\n",
      "613bddfaa4471bbe889926ee_pr_task_fullstudy1_2022-08-24_13h23.35.566.csv\n",
      "60684f29dbfe1bb2059e5e27_pr_task_fullstudy1_2022-08-24_21h16.21.751.csv\n",
      "5ed1463ba41e260775f2ec6c_pr_task_fullstudy1_2022-08-24_19h16.40.337.csv\n",
      "61687ebcd2a35ffb762d1928_pr_task_fullstudy1_2022-08-24_20h18.24.017.csv\n",
      "5efef32a88bb6a01c8087591_pr_task_fullstudy1_2022-08-25_01h15.15.289.csv\n",
      "6075cf9b059725b365f85e34_pr_task_fullstudy1_2022-08-24_20h19.59.200.csv\n",
      "611cf957c6a72bb4143ecd4a_pr_task_fullstudy1_2022-08-24_20h47.37.963.csv\n",
      "5f653cb18aad310a9ee7c32d_pr_task_fullstudy1_2022-08-24_19h16.40.440.csv\n",
      "6158ba08f68a4b368602a5d8_pr_task_fullstudy1_2022-08-24_20h16.40.659.csv\n",
      "608aa72c95a49d6f70deb05e_pr_task_fullstudy1_2022-08-24_20h16.01.728.csv\n",
      "608702d7661a76c56f62a02a_pr_task_fullstudy1_2022-08-24_19h20.08.276.csv\n",
      "60d91337ab2bb56833f78998_pr_task_fullstudy1_2022-08-24_19h16.09.535.csv\n",
      "5d87296392bb88000182f43a_pr_task_fullstudy1_2022-08-24_19h18.02.076.csv\n",
      "5e822d17f956a29530ccc2f8_pr_task_fullstudy1_2022-08-24_21h08.26.370.csv\n",
      "5ea6e7dff5d217088a20deb0_pr_task_fullstudy1_2022-08-24_21h03.46.248.csv\n",
      "60d2c0f8a8307cd0d6ccf5eb_pr_task_fullstudy1_2022-08-24_19h18.01.998.csv\n",
      "5b9d57e5737d030001ad2cbf_pr_task_fullstudy1_2022-08-24_19h19.55.201.csv\n",
      "62b8cd151b5fd0f9be7d1f11_pr_task_fullstudy1_2022-08-24_20h43.53.412.csv\n",
      "5eceef5fa487421604c337ba_pr_task_fullstudy1_2022-08-24_22h40.31.157.csv\n",
      "5d6ed96c3fe1ac001add3be8_pr_task_fullstudy1_2022-08-24_19h18.09.223.csv\n",
      "5e4b2a1a6a0c8403b9f8938e_pr_task_fullstudy1_2022-08-24_20h17.37.827.csv\n",
      "5e8b9e5e08c9f207f612ac74_pr_task_fullstudy1_2022-08-24_21h04.37.808.csv\n",
      "5fd28d6a17926504442d501c_pr_task_fullstudy1_2022-08-24_20h20.36.998.csv\n",
      "62e0251cc2818b23981bfa87_pr_task_fullstudy1_2022-08-24_19h23.21.206.csv\n",
      "5d40a12f4994c40001e4b80c_pr_task_fullstudy1_2022-08-24_20h16.46.177.csv\n",
      "5fbb9d170045d6d085a9bbbf_pr_task_fullstudy1_2022-08-24_20h25.29.977.csv\n",
      "60534ad8f7872caee17c7a96_pr_task_fullstudy1_2022-08-24_18h17.19.014.csv\n"
     ]
    }
   ],
   "source": [
    "neg_rwd_thresh=0.0000001\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "subs = [x for x in os.listdir(os.curdir) if x.startswith('5') or x.startswith('6')]\n",
    "\n",
    "# Query type number 1 -- vanilla PR vs. vanilla SR, latter has EV of 0 \n",
    "PRvsGuess={2:[['fox','houses','train'],{'fox':138,'houses':215,'train':168}],\n",
    "          11:[['fox','houses','train'],{'fox':138,'houses':215,'train':168}],\n",
    "          13:[['fox','houses','train'],{'fox':138,'houses':215,'train':168}],\n",
    "          15:[['fox','houses','train'],{'fox':138,'houses':215,'train':168}],\n",
    "          1:[['fox','houses','north'],{'fox':156,'houses':208,'north':137}],\n",
    "          3:[['fox','houses','north'],{'fox':156,'houses':208,'north':137}],\n",
    "          4:[['fox','houses','north'],{'fox':156,'houses':208,'north':137}],\n",
    "          5:[['fox','houses','north'],{'fox':156,'houses':208,'north':137}]}\n",
    "\n",
    "\n",
    "# check if participants have a bias for distal rewards\n",
    "DistalBias={17:[['tree','north'],{'tree':11,'north':130}],\n",
    "18:[['fox','houses'],{'fox':11,'houses':130}],\n",
    "21:[['watch','compass'],{'watch':50,'compass':4}],\n",
    "22:[['bell','snorkel'],{'bell':50,'snorkel':4}],\n",
    "25:[['tree','train'],{'tree':48,'train':4}],\n",
    "26:[['fox','thermometer'],{'fox':48,'thermometer':4}],\n",
    "29:[['bell','compass','snorkel','watch'],{'bell':10,'compass':126,'snorkel':1,'watch':1}],\n",
    "30:[['bell','compass','snorkel','watch'],{'watch':10,'snorkel':126,'compass':1,'bell':1}]\n",
    "}\n",
    "\n",
    "# reward revaluation queries\n",
    "RewardRevaluation={1:[['houses'],{'houses':100}],\n",
    "                   2:[['train'],{'train':100}],\n",
    "                   3:[['snorkel'],{'snorkel':100}],\n",
    "                   4:[['tophat'],{'tophat':100}]}\n",
    "\n",
    "state_dictionary={\n",
    "#starting state\n",
    "'start':0,\n",
    "#second stage\n",
    "'trident':2,'planet':3,    \n",
    "'bell':4,'tree':5,'watch':6,'fox':7,  \n",
    "'houses':8,'compass':9,'train':10,'thermometer':11,'microphone':12,'tophat':13,'north':14,'snorkel':15}\n",
    "\n",
    "sr_matrix='SR_matrix.npy'\n",
    "pr_matrix='PR_matrix.npy'\n",
    "rr_key='key_resp_33.keys'\n",
    "\n",
    "one_many=['many','one']\n",
    "one_type=['rwd','prob','ev']\n",
    "neglect_rwd=['','neg_rwd']\n",
    "neglect_rare_transition=['','neg_rare']\n",
    "round_ev=[0,1]\n",
    "normalize_max=['','norm']\n",
    "\n",
    "all_dfs={}\n",
    "counter=0\n",
    "\n",
    "for sub in subs:\n",
    "    print(sub)\n",
    "    df=pd.read_csv(sub)\n",
    "    rd=get_relative_frequencies(sub)\n",
    "    df_queries=df[df['trial_num'].notnull() & df['PRG_vs_Guessing.thisIndex'].notnull()]\n",
    "    df_queries=df_queries[df_queries['trial_num'].isin(list(PRvsGuess.keys()))]\n",
    "    df_queries=df_queries.reset_index(drop=True)\n",
    "    df_queries['sub_num']=counter\n",
    "\n",
    "    df_queries2=df[df['trial_num'].notnull() & df['PRG_vs_Guessing.thisIndex'].notnull()]\n",
    "    df_queries2=df_queries2[df_queries2['trial_num'].isin(list(DistalBias.keys()))]\n",
    "    df_queries2=df_queries2.reset_index(drop=True)\n",
    "    df_queries2['sub_num']=counter\n",
    "\n",
    "\n",
    "    df_queries3=df[df['trial_num'].notnull()]\n",
    "    df_queries3=df_queries3[df_queries3['reward_reval_real.thisTrialN'].isin([int(x)-1 for x in list(RewardRevaluation.keys())])]\n",
    "    df_queries3=df_queries3.reset_index(drop=True)\n",
    "    df_queries3['sub_num']=counter\n",
    "\n",
    "    for o_m in one_many:\n",
    "        for n_rwd in neglect_rwd:\n",
    "                for n_rare in neglect_rare_transition:\n",
    "                    for norm in normalize_max:\n",
    "\n",
    "                        if o_m=='one':\n",
    "                            for one_t in one_type:\n",
    "\n",
    "                                ratios_goals_pr={}\n",
    "                                ratios_goals_sr={}\n",
    "                                goal_cols=['r1','r2','r4']\n",
    "\n",
    "                                pr_ev_diff=[]\n",
    "                                sr_ev_diff=[]\n",
    "                                pr_a1p=[]\n",
    "                                pr_a0p=[]\n",
    "                                pr_a1r=[]\n",
    "                                pr_a0r=[]\n",
    "\n",
    "                                sr_a1p=[]\n",
    "                                sr_a0p=[]\n",
    "                                sr_a1r=[]\n",
    "                                sr_a0r=[]\n",
    "\n",
    "                                choices=[0 if x==9 else x for x in df_queries['key_resp_25.keys']]\n",
    "                                df_queries['choices']=choices\n",
    "\n",
    "\n",
    "                                #tn is trial number\n",
    "                                for tn in df_queries['trial_num']:\n",
    "\n",
    "                                    rewards=PRvsGuess[tn][1]\n",
    "                                    \n",
    "                                    ratios_action1_pr=get_specific_ratio(rd,'planet',PRvsGuess[tn][0],'pr')\n",
    "                                    ratios_action0_pr=get_specific_ratio(rd,'trident',PRvsGuess[tn][0],'pr')\n",
    "                                    ratios_action1_sr=get_specific_ratio(rd,'planet',PRvsGuess[tn][0],'sr')\n",
    "                                    ratios_action0_sr=get_specific_ratio(rd,'trident',PRvsGuess[tn][0],'sr')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                    action1_sr_probabilities,action1_sr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_sr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                    sr_a1p.append(action1_sr_probabilities)\n",
    "                                    sr_a1r.append(action1_sr_rewards)\n",
    "                                    action0_sr_probabilities,action0_sr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_sr,sr_matrix=sr_matrix,\n",
    "                                                           pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                                   one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                                  neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                    sr_a0p.append(action0_sr_probabilities)\n",
    "                                    sr_a0r.append(action0_sr_rewards)\n",
    "                                    action1_pr_probabilities,action1_pr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_pr,sr_matrix=sr_matrix,\n",
    "                                                           pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                                   one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                                  neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                    pr_a1p.append(action1_pr_probabilities)\n",
    "                                    pr_a1r.append(action1_pr_rewards)\n",
    "\n",
    "                                    action0_pr_probabilities,action0_pr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_pr,sr_matrix=sr_matrix,\n",
    "                                                           pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                                   one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                                  neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "\n",
    "\n",
    "                                    pr_a0p.append(action0_pr_probabilities)\n",
    "                                    pr_a0r.append(action0_pr_rewards)\n",
    "                                for i in range(4):\n",
    "                                    df_queries['pr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1p[j][i] for j in range(8)]\n",
    "                                    df_queries['pr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0p[j][i] for j in range(8)]\n",
    "                                    df_queries['pr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1r[j][i] for j in range(8)]\n",
    "                                    df_queries['pr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0r[j][i] for j in range(8)]\n",
    "\n",
    "                                    df_queries['sr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1p[j][i] for j in range(8)]\n",
    "                                    df_queries['sr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0p[j][i] for j in range(8)]\n",
    "                                    df_queries['sr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1r[j][i] for j in range(8)]\n",
    "                                    df_queries['sr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0r[j][i] for j in range(8)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                ratios_goals_pr={}\n",
    "                                ratios_goals_sr={}\n",
    "                                goal_cols=['r1','r2','r4']\n",
    "                                pr_ev_diff=[]\n",
    "                                sr_ev_diff=[]\n",
    "                                pr_a1p=[]\n",
    "                                pr_a0p=[]\n",
    "                                pr_a1r=[]\n",
    "                                pr_a0r=[]\n",
    "\n",
    "                                sr_a1p=[]\n",
    "                                sr_a0p=[]\n",
    "                                sr_a1r=[]\n",
    "                                sr_a0r=[]\n",
    "\n",
    "                                choices=[0 if x==9 else x for x in df_queries2['key_resp_25.keys']]\n",
    "                                df_queries2['choices']=choices\n",
    "                                #tn is trial number\n",
    "                                for tn in df_queries2['trial_num']:\n",
    "\n",
    "\n",
    "                                    rewards=DistalBias[tn][1]\n",
    "                                    ratios_action1_pr=get_specific_ratio(rd,'planet',DistalBias[tn][0],'pr')\n",
    "                                    ratios_action0_pr=get_specific_ratio(rd,'trident',DistalBias[tn][0],'pr')\n",
    "                                    ratios_action1_sr=get_specific_ratio(rd,'planet',DistalBias[tn][0],'sr')\n",
    "                                    ratios_action0_sr=get_specific_ratio(rd,'trident',DistalBias[tn][0],'sr')\n",
    "                                    action1_sr_probabilities,action1_sr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_sr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                    sr_a1p.append(action1_sr_probabilities)\n",
    "                                    sr_a1r.append(action1_sr_rewards)\n",
    "                                    action0_sr_probabilities,action0_sr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_sr,sr_matrix=sr_matrix,\n",
    "                                                           pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                                   one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                                  neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                    sr_a0p.append(action0_sr_probabilities)\n",
    "                                    sr_a0r.append(action0_sr_rewards)\n",
    "                                    action1_pr_probabilities,action1_pr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_pr,sr_matrix=sr_matrix,\n",
    "                                                           pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                                   one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                                  neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                    pr_a1p.append(action1_pr_probabilities)\n",
    "                                    pr_a1r.append(action1_pr_rewards)\n",
    "\n",
    "                                    action0_pr_probabilities,action0_pr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_pr,sr_matrix=sr_matrix,\n",
    "                                                           pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                                   one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                                  neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "\n",
    "\n",
    "                                    pr_a0p.append(action0_pr_probabilities)\n",
    "                                    pr_a0r.append(action0_pr_rewards)\n",
    "                                for i in range(4):\n",
    "                                    df_queries2['pr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1p[j][i] for j in range(8)]\n",
    "                                    df_queries2['pr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0p[j][i] for j in range(8)]\n",
    "                                    df_queries2['pr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1r[j][i] for j in range(8)]\n",
    "                                    df_queries2['pr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0r[j][i] for j in range(8)]\n",
    "\n",
    "                                    df_queries2['sr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1p[j][i] for j in range(8)]\n",
    "                                    df_queries2['sr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0p[j][i] for j in range(8)]\n",
    "                                    df_queries2['sr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1r[j][i] for j in range(8)]\n",
    "                                    df_queries2['sr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0r[j][i] for j in range(8)]\n",
    "\n",
    "\n",
    "                                ratios_goals_pr={}\n",
    "                                ratios_goals_sr={}\n",
    "                                goal_cols=['r1','r2','r4']\n",
    "                                pr_ev_diff=[]\n",
    "                                sr_ev_diff=[]\n",
    "                                choices=[0 if x==9 else x for x in df_queries3['key_resp_31.keys']]\n",
    "                                df_queries3['choices']=choices\n",
    "                                conversion_dict={1:1,2:2,3:3,4:4,5:1,6:2,7:3,8:4}\n",
    "                                pr_a1p=[]\n",
    "                                pr_a0p=[]\n",
    "                                pr_a1r=[]\n",
    "                                pr_a0r=[]\n",
    "\n",
    "                                sr_a1p=[]\n",
    "                                sr_a0p=[]\n",
    "                                sr_a1r=[]\n",
    "                                sr_a0r=[]\n",
    "                                #tn is trial number\n",
    "                                for tn in df_queries3['trial_num']:\n",
    "                                    tn=conversion_dict[tn]\n",
    "                                    rewards=RewardRevaluation[tn][1]\n",
    "                                    ratios_action1_pr=get_specific_ratio(rd,'planet',RewardRevaluation[tn][0],'pr')\n",
    "                                    ratios_action0_pr=get_specific_ratio(rd,'trident',RewardRevaluation[tn][0],'pr')\n",
    "                                    ratios_action1_sr=get_specific_ratio(rd,'planet',RewardRevaluation[tn][0],'sr')\n",
    "                                    ratios_action0_sr=get_specific_ratio(rd,'trident',RewardRevaluation[tn][0],'sr')\n",
    "\n",
    "\n",
    "                                    action1_sr_probabilities,action1_sr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_sr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                    sr_a1p.append(action1_sr_probabilities)\n",
    "                                    sr_a1r.append(action1_sr_rewards)\n",
    "                                    action0_sr_probabilities,action0_sr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_sr,sr_matrix=sr_matrix,\n",
    "                                                           pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                                   one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                                  neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                    sr_a0p.append(action0_sr_probabilities)\n",
    "                                    sr_a0r.append(action0_sr_rewards)\n",
    "                                    action1_pr_probabilities,action1_pr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_pr,sr_matrix=sr_matrix,\n",
    "                                                           pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                                   one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                                  neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                    pr_a1p.append(action1_pr_probabilities)\n",
    "                                    pr_a1r.append(action1_pr_rewards)\n",
    "\n",
    "                                    action0_pr_probabilities,action0_pr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_pr,sr_matrix=sr_matrix,\n",
    "                                                           pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                                   one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                                  neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "\n",
    "\n",
    "                                    pr_a0p.append(action0_pr_probabilities)\n",
    "                                    pr_a0r.append(action0_pr_rewards)\n",
    "                                for i in range(4):\n",
    "                                    df_queries3['pr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1p[j][i] for j in range(4)]\n",
    "                                    df_queries3['pr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0p[j][i] for j in range(4)]\n",
    "                                    df_queries3['pr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1r[j][i] for j in range(4)]\n",
    "                                    df_queries3['pr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0r[j][i] for j in range(4)]\n",
    "\n",
    "                                    df_queries3['sr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1p[j][i] for j in range(4)]\n",
    "                                    df_queries3['sr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0p[j][i] for j in range(4)]\n",
    "                                    df_queries3['sr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1r[j][i] for j in range(4)]\n",
    "                                    df_queries3['sr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0r[j][i] for j in range(4)]\n",
    "\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            one_t='ev'\n",
    "                            ratios_goals_pr={}\n",
    "                            ratios_goals_sr={}\n",
    "                            goal_cols=['r1','r2','r4']\n",
    "\n",
    "                            pr_ev_diff=[]\n",
    "                            sr_ev_diff=[]\n",
    "\n",
    "                            choices=[0 if x==9 else x for x in df_queries['key_resp_25.keys']]\n",
    "                            df_queries['choices']=choices\n",
    "                            pr_a1p=[]\n",
    "                            pr_a0p=[]\n",
    "                            pr_a1r=[]\n",
    "                            pr_a0r=[]\n",
    "                            \n",
    "                            sr_a1p=[]\n",
    "                            sr_a0p=[]\n",
    "                            sr_a1r=[]\n",
    "                            sr_a0r=[]\n",
    "                            #tn is trial number\n",
    "                            for tn in df_queries['trial_num']:\n",
    "\n",
    "                                rewards=PRvsGuess[tn][1]\n",
    "                                ratios_action1_pr=get_specific_ratio(rd,'planet',PRvsGuess[tn][0],'pr')\n",
    "                                ratios_action0_pr=get_specific_ratio(rd,'trident',PRvsGuess[tn][0],'pr')\n",
    "                                ratios_action1_sr=get_specific_ratio(rd,'planet',PRvsGuess[tn][0],'sr')\n",
    "                                ratios_action0_sr=get_specific_ratio(rd,'trident',PRvsGuess[tn][0],'sr')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                action1_sr_probabilities,action1_sr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_sr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                action1_sr_probabilities=[0 for x in action1_sr_probabilities]\n",
    "                                sr_a1p.append(action1_sr_probabilities)\n",
    "                                sr_a1r.append(action1_sr_rewards)\n",
    "                                action0_sr_probabilities,action0_sr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_sr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                action0_sr_probabilities=[0 for x in action0_sr_probabilities]\n",
    "                                sr_a0p.append(action0_sr_probabilities)\n",
    "                                sr_a0r.append(action0_sr_rewards)\n",
    "                                action1_pr_probabilities,action1_pr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_pr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                pr_a1p.append(action1_pr_probabilities)\n",
    "                                pr_a1r.append(action1_pr_rewards)\n",
    "                                \n",
    "                                action0_pr_probabilities,action0_pr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_pr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "\n",
    "                         \n",
    "                                pr_a0p.append(action0_pr_probabilities)\n",
    "                                pr_a0r.append(action0_pr_rewards)\n",
    "\n",
    "                            for i in range(4):\n",
    "                                df_queries['pr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1p[j][i] for j in range(8)]\n",
    "                                df_queries['pr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0p[j][i] for j in range(8)]\n",
    "                                df_queries['pr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1r[j][i] for j in range(8)]\n",
    "                                df_queries['pr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0r[j][i] for j in range(8)]\n",
    "\n",
    "                                df_queries['sr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1p[j][i] for j in range(8)]\n",
    "                                df_queries['sr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0p[j][i] for j in range(8)]\n",
    "                                df_queries['sr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1r[j][i] for j in range(8)]\n",
    "                                df_queries['sr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0r[j][i] for j in range(8)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            ratios_goals_pr={}\n",
    "                            ratios_goals_sr={}\n",
    "                            goal_cols=['r1','r2','r4']\n",
    "                            pr_ev_diff=[]\n",
    "                            sr_ev_diff=[]\n",
    "\n",
    "                            choices=[0 if x==9 else x for x in df_queries2['key_resp_25.keys']]\n",
    "                            df_queries2['choices']=choices\n",
    "                            pr_a1p=[]\n",
    "                            pr_a0p=[]\n",
    "                            pr_a1r=[]\n",
    "                            pr_a0r=[]\n",
    "                            \n",
    "                            sr_a1p=[]\n",
    "                            sr_a0p=[]\n",
    "                            sr_a1r=[]\n",
    "                            sr_a0r=[]\n",
    "                            #tn is trial number\n",
    "                            for tn in df_queries2['trial_num']:\n",
    "\n",
    "                                rewards=DistalBias[tn][1]\n",
    "                                ratios_action1_pr=get_specific_ratio(rd,'planet',DistalBias[tn][0],'pr')\n",
    "                                ratios_action0_pr=get_specific_ratio(rd,'trident',DistalBias[tn][0],'pr')\n",
    "                                ratios_action1_sr=get_specific_ratio(rd,'planet',DistalBias[tn][0],'sr')\n",
    "                                ratios_action0_sr=get_specific_ratio(rd,'trident',DistalBias[tn][0],'sr')\n",
    "\n",
    "                                action1_sr_probabilities,action1_sr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_sr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                sr_a1p.append(action1_sr_probabilities)\n",
    "                                sr_a1r.append(action1_sr_rewards)\n",
    "                                action0_sr_probabilities,action0_sr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_sr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                sr_a0p.append(action0_sr_probabilities)\n",
    "                                sr_a0r.append(action0_sr_rewards)\n",
    "                                action1_pr_probabilities,action1_pr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_pr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                pr_a1p.append(action1_pr_probabilities)\n",
    "                                pr_a1r.append(action1_pr_rewards)\n",
    "                                \n",
    "                                action0_pr_probabilities,action0_pr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_pr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "\n",
    "                         \n",
    "                                pr_a0p.append(action0_pr_probabilities)\n",
    "                                pr_a0r.append(action0_pr_rewards)\n",
    "                            for i in range(4):\n",
    "                                df_queries2['pr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1p[j][i] for j in range(8)]\n",
    "                                df_queries2['pr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0p[j][i] for j in range(8)]\n",
    "                                df_queries2['pr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1r[j][i] for j in range(8)]\n",
    "                                df_queries2['pr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0r[j][i] for j in range(8)]\n",
    "\n",
    "                                df_queries2['sr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1p[j][i] for j in range(8)]\n",
    "                                df_queries2['sr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0p[j][i] for j in range(8)]\n",
    "                                df_queries2['sr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1r[j][i] for j in range(8)]\n",
    "                                df_queries2['sr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0r[j][i] for j in range(8)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            ratios_goals_pr={}\n",
    "                            ratios_goals_sr={}\n",
    "                            goal_cols=['r1','r2','r4']\n",
    "                            pr_ev_diff=[]\n",
    "                            sr_ev_diff=[]\n",
    "                            choices=[0 if x==9 else x for x in df_queries3['key_resp_31.keys']]\n",
    "                            df_queries3['choices']=choices\n",
    "                            conversion_dict={1:1,2:2,3:3,4:4,5:1,6:2,7:3,8:4}\n",
    "                            pr_a1p=[]\n",
    "                            pr_a0p=[]\n",
    "                            pr_a1r=[]\n",
    "                            pr_a0r=[]\n",
    "                            \n",
    "                            sr_a1p=[]\n",
    "                            sr_a0p=[]\n",
    "                            sr_a1r=[]\n",
    "                            sr_a0r=[]\n",
    "                            #tn is trial number\n",
    "                            for tn in df_queries3['trial_num']:\n",
    "                                tn=conversion_dict[tn]\n",
    "                                rewards=RewardRevaluation[tn][1]\n",
    "                                ratios_action1_pr=get_specific_ratio(rd,'planet',RewardRevaluation[tn][0],'pr')\n",
    "                                ratios_action0_pr=get_specific_ratio(rd,'trident',RewardRevaluation[tn][0],'pr')\n",
    "                                ratios_action1_sr=get_specific_ratio(rd,'planet',RewardRevaluation[tn][0],'sr')\n",
    "                                ratios_action0_sr=get_specific_ratio(rd,'trident',RewardRevaluation[tn][0],'sr')\n",
    "\n",
    "                                action1_sr_probabilities,action1_sr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_sr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                sr_a1p.append(action1_sr_probabilities)\n",
    "                                sr_a1r.append(action1_sr_rewards)\n",
    "                                action0_sr_probabilities,action0_sr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_sr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='sr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                sr_a0p.append(action0_sr_probabilities)\n",
    "                                sr_a0r.append(action0_sr_rewards)\n",
    "                                action1_pr_probabilities,action1_pr_rewards=individual_av_components(action=1,goals_reward=rewards,goals_ratio=ratios_action1_pr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "                                pr_a1p.append(action1_pr_probabilities)\n",
    "                                pr_a1r.append(action1_pr_rewards)\n",
    "                                \n",
    "                                action0_pr_probabilities,action0_pr_rewards=individual_av_components(action=0,goals_reward=rewards,goals_ratio=ratios_action0_pr,sr_matrix=sr_matrix,\n",
    "                                                       pr_matrix=pr_matrix,state_map=state_dictionary,\n",
    "                                               one_or_many=o_m,one_type=one_t,sr_or_pr='pr',neglect_reward=n_rwd,\n",
    "                                              neglect_rare_transition=n_rare,inverse_prior_confidence=0,sr_pr_weighting=0.5,range_adapt=norm,nr_t=neg_rwd_thresh)\n",
    "\n",
    "                         \n",
    "                                pr_a0p.append(action0_pr_probabilities)\n",
    "                                pr_a0r.append(action0_pr_rewards)\n",
    "                        \n",
    "                            for i in range(4):\n",
    "                                df_queries3['pr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1p[j][i] for j in range(4)]\n",
    "                                df_queries3['pr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0p[j][i] for j in range(4)]\n",
    "                                df_queries3['pr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a1r[j][i] for j in range(4)]\n",
    "                                df_queries3['pr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[pr_a0r[j][i] for j in range(4)]\n",
    "\n",
    "                                df_queries3['sr_{}_{}_{}_{}_{}_prob{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1p[j][i] for j in range(4)]\n",
    "                                df_queries3['sr_{}_{}_{}_{}_{}_prob{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0p[j][i] for j in range(4)]\n",
    "                                df_queries3['sr_{}_{}_{}_{}_{}_rwd{}_action1'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a1r[j][i] for j in range(4)]\n",
    "                                df_queries3['sr_{}_{}_{}_{}_{}_rwd{}_action0'.format(o_m,n_rwd,n_rare,norm,one_t,i)]=[sr_a0r[j][i] for j in range(4)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    all_dfs['sub_{}'.format(counter)]=pd.concat([df_queries,df_queries2,df_queries3])\n",
    "    counter+=1\n",
    "\n",
    "dall=pd.concat([x for x in all_dfs.values()])\n",
    "dall=dall.reset_index(drop=True)\n",
    "good_cols=[x for x in dall.columns if x.startswith('pr_') or x.startswith ('sr_')]\n",
    "good_cols.append('sub_num')\n",
    "good_cols.append('choices')\n",
    "dall=dall[good_cols]\n",
    "retain_cols=['sub_num','choices']\n",
    "\n",
    "\n",
    "\n",
    "filtered_df1 = dall.loc[dall['pr_many____ev_rwd0_action1'].isin([46, 52]),:]\n",
    "filtered_df2 = dall.loc[~dall['pr_many____ev_rwd0_action1'].isin([46, 52]),:]\n",
    "\n",
    "filtered_df1.to_csv('all_subs_Study1_actionvaluesrounded_PRvsGuessing.csv')\n",
    "filtered_df2.to_csv('all_subs_Study1_actionvaluesrounded_restofqueries.csv',encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58f27aa",
   "metadata": {},
   "source": [
    "# Run Analysis in R\n",
    "\n",
    "- Use file model_fitting.R, and then immediately after, model_fitting_rest.R, in directory to run model comparison. Uses's BRM's bridgesampling algorithm to obtain reliable estimates of each model's log likelihood.\n",
    "- This will need to be done externally in an R terminal\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
